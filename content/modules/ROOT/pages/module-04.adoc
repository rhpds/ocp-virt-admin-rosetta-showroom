= Affinity and Anti-Affinity for VM Placement

This lab showcases how to use and apply Node Affinity, Pod Affinity, and Pod-Anti Affinity to Virtual Machines. The lab focuses on the practical application of these principles in real-world scenarios and strengthens your technical understanding of how they work.

[NOTE]
====
This feature is similar to VMWare's Virtual Machine Affinity Rules without DRS.

You can combine this feature, with the *Descheduler* using the *AffinityAndTaints* profile to create a similar *Virtual Machine Affinity Rules* with *DRS* experience.
====

include::lab-access.adoc[]

[#nodeaffinity]
== Node Affinity

Node Affinity is a set of rules that guide the scheduler to attract a Virtual Machine to a specific node or group of nodes. These rules rely on matching labels that are applied to the nodes.

The core use case for Node Affinity is to ensure that a VM runs only on nodes that possess specific features or needs, such as a particular GPU model or a high amount of RAM, by matching the corresponding labels on the node.

This lab will demonstrate how Node Affinity is set up and how it functions.

[#nodeaffinityinstructions]
=== Instructions

. Ensure you are <<labaccess,logged in to both the OpenShift Console and CLI as the *admin* user>> from your *web browser* and the *terminal* window on the right side of your screen and continue to the next step.

+

. Start the *node-affinity-vm* Virtual Machine

+
[.wrap,console,role=execute]
----
virtctl start node-affinity-vm -n affinity
----

+
[source,console]
----
VM node-affinity-vm was scheduled to start
----

+
. Verify the *VirtualMachineInstance* is running and see what node it is running on.
+
[.wrap,console,role=execute]
----
oc get vmi -n affinity
----

+
The output will look similar to the following, with a different *IP* and *NODENAME*:

+
[source,console]
----
NAME               AGE   PHASE     IP             NODENAME                        READY
node-affinity-vm   57s   Running   10.232.1.153   control-plane-cluster-dvddt-1   True
----

+
. Set a label `zone=east` on any node where the *node-affinity-vm* is currently *not* running.

+
The following command returns the name of 1 node where the *node-affinity-vm* VM is not running and labels it:

+
... Gets a list of all of nodes (*oc get nodes -o name*) 

+
... Does an inverted-match to select non-matching lines and returns 1 result (*grep -v -m 1*) 

+
... With the *nodeName* where the *node-affinity-vm* VM is currently running (*oc get vmi node-affinity-vm -n affinity -o json | jq -r '.status.nodeName'*) 

+
[source,text,role=execute]
.Set the label
----
oc label $(oc get nodes -o name | grep -v -m 1 $(oc get vmi node-affinity-vm -n affinity -o json | jq -r '.status.nodeName')) zone=east
----

+
.Output
----
node/worker-cluster-dvddt-1 labeled
----

+
Alternatively, using the OpenShift Console, from the left side panel, navigate to *Compute → Nodes*, pick a node where the *node-affinity-vm* is not running, click the 3 dots and select *Edit labels*.

+
image::exercise4/04-image-affinity-v2.png[title="Edit Node Labels", link=self, window=blank, width=100%]

+
In the new window, enter *zone=east* and click *Save*

+
image::exercise4/04-image-affinity-labels.png[title="Add Label", link=self, window=blank, width=100%]

+
. Make sure the label was applied.

+
[source,text,role=execute]
.Check the label
----
oc get nodes <worker you just labeled> --show-labels | grep -i zone=east
----

+
Alternatively, using the OpenShift Console, from the left side panel, navigate to *Compute → Nodes*, select the node where you added the label, click the *Details* tab and check the *Labels* section for *zone=east*.

+
image::exercise4/04-image-affinity-check-labels.png[title="Check the Label", link=self, window=blank, width=100%]

+
. Using the OpenShift Console, from the left side panel, navigate to *Virtualization → VirtualMachines*.

+
Under *All projects*, select the *affinity* namespace and click on the Virtual Machine named *node-affinity-vm*. → Configuration* 

+
. Click on the *Configuration* tab.

+
image::exercise4/04-image-affinity01.png[title="Node Affinity Navigation", link=self, window=blank, width=100%]

+
. From the *Configuration* tab, select *Scheduling* and click the *blue pencil* icon under *Affinity rules* to add a new one.

+
image::exercise4/04-image-affinity02.png[title="Node Affinity add rule", link=self, window=blank, width=100%]

+
. Click *Add affinity rule*.

+
image::exercise4/04-image-affinity02a.png[title="Node Affinity add rule", link=self, window=blank, width=100%]

+
. Change the *Condition* to *Preferred during scheduling* and set the *weight* to *75*.

+
Under *Node Labels* click *Add Expression*.

+
Set the *Key* field to `zone` and the *Values* field to `east` and click *Add*.

+
This is the same label applied to the node earler.

+
Your Final Node Affinity rule will look like the picture below.

+
Confirm this is true and click *Save affinity rule*.

+
image::exercise4/04-image-affinity03.png[title="Node Affinity Rule", link=self, window=blank, width=100%]

+
. Click *Apply rules*.

+
image::exercise4/04-image-affinity03a.png[title="Node Affinity Rule", link=self, window=blank, width=100%]

+
. Let's take a look at the Node Affinity rule on *node-affinity-vm*.

+
You can view this information through the GUI by inspecting the *node-affinity-vm* *YAML*, or using the CLI.

+
[source,text,role=execute]
.View the VM affinity definition
----
oc get vm node-affinity-vm -n affinity -o jsonpath='{.spec.template.spec.affinity}{"\n"}'
----

+
.Output
----
{"nodeAffinity":{"preferredDuringSchedulingIgnoredDuringExecution":[{"preference":{"matchExpressions":[{"key":"zone","operator":"In","values":["east"]}]},"weight":75}]}}
----

+
. With an external force to move or restart the VM, new Affinity rules do not take effect.

+
To apply the changes manually, you can live migrate or restart the VM. For automatic enforcement, you can configure the *Descheduler* with the *AffinityAndTaints* profile.

+
. Restart the *node-affinity-vm* VM.

+
[.wrap,console,role=execute]
----
virtctl restart node-affinity-vm -n affinity
----

+
[source,console]
----
VM node-affinity-vm was scheduled to restart
----

+
. Once the VM restarts, it will be running on the node with the affinity label. 

+
You can validate this in the GUI by navigating to the *node-affinity-vm*, and looking at the *Node* name in the *General* box on the right hand side of the *VirtualMachine details* page or using the OCP CLI.

+
[source,text,role=execute]
.Get VMI information
----
oc get vmi node-affinity-vm -n affinity
---- 

+
.Output
----
NAME               AGE   PHASE     IP            NODENAME                 READY
node-affinity-vm   58s   Running   10.233.0.46   worker-cluster-t96sv-2   True
----

[#podaffinity]
== Pod Affinity
Pod Affinity is a scheduling rule that co-locates a VM Pod and an existing Pod (or another VM Pod) with a specific label onto the same node.

The primary benefit of using pod affinity is to improve performance for dependent VMs or services that require low-latency communication by guaranteeing their placement on the same worker node.

This lab will illustrate the setup and function of Pod Affinity.

[#podaffinityinstructions]
=== Instructions

[start=1]
. Ensure you are <<labaccess,logged in to both the OpenShift Console and CLI as the *admin* user>> from your *web browser* and the *terminal* window on the right side of your screen and continue to the next step.

+
. To begin, we need to apply a label to the *node-affinity-vm*. This will enable it to serve as the VM that our pod will have Pod Affinity to. The label can be added by editing the VM within the OpenShift CLI and including `app: fedora`

+
[source,text,role=execute]
.Modify the VM
----
oc edit vm node-affinity-vm -n affinity
----
+
[source,yaml,role=execute]
----
apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  annotations:
    argocd.argoproj.io/sync-options: SkipDryRunOnMissingResource=true
    argocd.argoproj.io/sync-wave: "10"
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"kubevirt.io/v1","kind":"VirtualMachine","metadata":{"annotations":{"argocd.argoproj.io/sync-options":"SkipDryRunOnMissingResource=true","argocd.argoproj.io/sync-wave":"10"},"labels":{"app.kubernetes.io/instance":"module-affinity"},"name":"node-affinity-vm","namespace":"affinity"},"spec":{"dataVolumeTemplates":[{"metadata":{"name":"node-affinity-vm-volume"},"spec":{"sourceRef":{"kind":"DataSource","name":"rhel10","namespace":"openshift-virtualization-os-images"},"storage":{"resources":{"requests":{"storage":"30Gi"}}}}}],"instancetype":{"name":"u1.small"},"preference":{"name":"rhel.10"},"runStrategy":"Manual","template":{"metadata":{"labels":{"network.kubevirt.io/headlessService":"headless"}},"spec":{"domain":{"devices":{"autoattachPodInterface":false,"disks":[{"disk":{"bus":"virtio"},"name":"cloudinitdisk"}],"interfaces":[{"masquerade":{},"name":"default"}]}},"networks":[{"name":"default","pod":{}}],"subdomain":"headless","volumes":[{"dataVolume":{"name":"node-affinity-vm-volume"},"name":"rootdisk"},{"cloudInitNoCloud":{"userData":"#cloud-config\nchpasswd:\n  expire: false\npassword: redhat\nuser: rhel\n"},"name":"cloudinitdisk"}]}}}}
    kubemacpool.io/transaction-timestamp: "2025-12-13T00:21:52.599027397Z"
    kubevirt.io/latest-observed-api-version: v1
    kubevirt.io/storage-observed-api-version: v1
  creationTimestamp: "2025-12-12T20:44:59Z"
  finalizers:
  - kubevirt.io/virtualMachineControllerFinalize
  generation: 2
  labels:
    app.kubernetes.io/instance: module-affinity <---Do not put the label here
  name: node-affinity-vm
  namespace: affinity
  resourceVersion: "328565"
  uid: 6e05ce41-019f-498a-a07f-ef4e9c93ddd3
spec:
  dataVolumeTemplates:
  - metadata:
      creationTimestamp: null
      name: node-affinity-vm-volume
    spec:
      sourceRef:
        kind: DataSource
        name: rhel10
        namespace: openshift-virtualization-os-images
      storage:
        resources:
          requests:
            storage: 30Gi
  instancetype:
    kind: virtualmachineclusterinstancetype
    name: u1.small
  preference:
    kind: virtualmachineclusterpreference
    name: rhel.10
  runStrategy: Manual
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: fedora   <---Put the label here
        network.kubevirt.io/headlessService: headless
----
+
[source,text,role=execute]
----
:wq!
----

+
. A reboot is required for the *node-affinity-vm* in order to apply the label to its vmi.

+
. Stop the *node-affinity-vm* VM by clicking on *Action → Stop*.

+
. Once the VM completely stops. Click *Actions → Start*.

+
. Click the *pod-affinity-vm*  

+
. Click *Configure → Scheduling*.

+
. Click the *Affinity rules*.

+
image::exercise4/04-image-affinity04.png[title="Pod Affinity Rule", link=self, window=blank, width=100%]

+
. Click *Add affinity rule*.

+
. Change the type to *Workload (pod) Affinity*

+
. Keep the *Condition* set to *Required during scheduling*.

+
. Leave the *Topology key* at the default value.

+
. Click *Add expression* under *Workload labels*.

+
. You will now set a key so this VM will run on the same node as *node-affinity-vm*. Set the *Key* field to `app` and the *Values* field to `fedora` and click *add*. Your Final Pod Affinity rule will look like the picture below. Confirm this is true and click *Save affinity rule*.

+
image::exercise4/04-image-affinity05.png[title="Pod Affinity Rule", link=self, window=blank, width=100%]

+
. Click *Apply rules*

+
. Now let's take a look at the Pod Affinity rule on *pod-affinity-vm*. You can view this information either through the GUI by navigating to *pod-affinity-vm → YAML*, or by using the CLI.

+
[source,text,role=execute]
----
oc get vm pod-affinity-vm -n affinity -o jsonpath='{.spec.template.spec.affinity}{"\n"}'
---- 

+
.Output
----
{"podAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":[{"labelSelector":{"matchExpressions":[{"key":"app","operator":"In","values":["fedora"]}]},"topologyKey":"kubernetes.io/hostname"}]}}
----

+
. See where the pod-affinity-vm and node-affinity-vm are running by looking at OpenShift Console or check on the OpenShift Command line. 

+
[source,text,role=execute]
----
oc get vmi pod-affinity-vm node-affinity-vm -n affinity
----

+
.Output
----
NAME               AGE   PHASE     IP            NODENAME                 READY
pod-affinity-vm    30m   Running   10.233.0.54   worker-cluster-t96sv-3   True
node-affinity-vm    3m   Running   10.233.0.46   worker-cluster-t96sv-2   True
----

+
. To make the VM follow these new Pod Affinity rules. You normally would migrate it to the node with the label. However, to show that this Pod Affinity rule is working. We are going to power off the vm completely and then power it back on. 

+
. Stop the *pod-affinity-vm* VM by clicking on *Action → Stop*.

+
. Once the VM completely stops. Click *Actions → Start*.

+
. See where the *pod-affinity-vm* and *node-affinity-vm* are running by looking at OpenShift Console or check on the OpenShift Command line. 

+
[source,text,role=execute]
----
oc get vmi pod-affinity-vm node-affinity-vm -n affinity
----

+
.Output
----
NAME               AGE   PHASE     IP            NODENAME                 READY
pod-affinity-vm     1m   Running   10.233.0.54   worker-cluster-t96sv-2   True
node-affinity-vm    4m   Running   10.233.0.46   worker-cluster-t96sv-2   True
----

+
. You will now see the the *pod-affinity-vm* is now running the same location as the *node-affinity-vm* because of the pod affinity rule.


[#podantiaffinity]
== Pod Anti-Affinity

Pod Anti-Affinity is a crucial feature for achieving High Availability (HA) in virtualized environments. It functions by instructing the scheduler to prevent the co-location of VM Pods on the same node if they possess a specific label.

The primary benefit of this rule is to enhance application resilience. For instance, by ensuring that VM Pods belonging to the same service (such as a database cluster) are distributed across different nodes, a failure in a single node will not cause an outage for the entire service.

This lab will guide you through setting up and demonstrating the functionality of Pod Anti-Affinity.

[#podantiaffinityinstructions]
=== Instructions

[start=1]
. Ensure you are <<labaccess,logged in to both the OpenShift Console and CLI as the *admin* user>> from your *web browser* and the *terminal* window on the right side of your screen and continue to the next step.

+
. We will be reusing the `app: fedora` label we created last time to create our Pod Anti-Affinity rule. This rule will ensure Pod Anti-Affinity with *node-affinity-vm*.

+
. Click the *pod-anti-affinity-vm*  

+
. Click *Configure → Scheduling*

+
. Click the *Affinity rules*

+
. *Add Affinity rule*

+
image::exercise4/04-image-affinity06.png[title="Add Pod Anti-Affinity Rule", link=self, window=blank, width=100%]

+
. Change the type to *Workload (pod) Anti-Affinity*

+
. Keep the *Condition* set to *Required during scheduling*.

+
. Keep the *Topology key* the same 

+
. Click *Add expression* under *Workload labels*

+
. Next, we will set a *key* and *value* to establish Pod Anti-Affinity, ensuring this VM runs on a different node than *node-affinity-vm*. Set the *key* to `app` and the *value* to `fedora` and click *add*. The resulting Pod Affinity rule should match the illustration provided below. Verify the rule's correctness, then click *Save affinity rule*.

+
image::exercise4/04-image-affinity07.png[title="Add Pod Anti-Affinity Rule", link=self, window=blank, width=100%]

+
. Click *Apply Rules*

+
. Now let's take a look at the Pod Anti-Affinity rule on *pod-anti-affinity-vm*. You can view this information either through the GUI by navigating to *pod-anti-affinity-vm → YAML*, or by using the CLI.

+
[source,text,role=execute]
----
oc get vm pod-anti-affinity-vm -n affinity -o jsonpath='{.spec.template.spec.affinity}{"\n"}'
----

+
.Output
----
{"podAntiAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":[{"labelSelector":{"matchExpressions":[{"key":"app","operator":"In","values":["fedora"]}]},"topologyKey":"kubernetes.io/hostname"}]}}
----

+
. See where the *pod-anti-affinity-vm* and *node-affinity-vm* are running by looking at OpenShift Console or check on the OpenShift Command line. 

+
[source,text,role=execute]
----
oc get vmi pod-anti-affinity-vm node-affinity-vm -n affinity
----

+
.Output
----
NAME                   AGE   PHASE     IP            NODENAME                 READY
pod-anti-affinity-vm   30m   Running   10.233.0.44   worker-cluster-t96sv-2   True
node-affinity-vm        5m   Running   10.233.0.46   worker-cluster-t96sv-2   True
----

+
. To make the VM follow these new Pod Anti-Affinity rules. You normally would migrate it to the node with the label. However, to show that this Pod Anti-Affinity rule is working. We are going to power off the vm completely and then power it back on.

+
. Stop the *pod-anti-affinity-vm* VM by clicking *Action → Stop*.

+
. Once the VM completely stops. Click *Actions → Start*.

+
. See where the *pod-anti-affinity-vm* and *node-affinity-vm* are running by looking at OpenShift Console or check on the OpenShift Command line. 

+
[source,text,role=execute]
----
oc get vmi pod-anti-affinity-vm node-affinity-vm -n affinity
----

+
.Output
----
NAME                   AGE   PHASE     IP            NODENAME                 READY
pod-anti-affinity-vm    1m   Running   10.233.0.44   worker-cluster-t96sv-3   True
node-affinity-vm        6m   Running   10.233.0.46   worker-cluster-t96sv-2   True
----

+
. The *pod-anti-affinity-vm* VM will now be running on a different node than *node-affinity-vm*. If *pod-anti-affinity-vm* was not already running on a different node than the *node-affinity-vm*. 
